{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä Web Scraper Simple\n",
                "\n",
                "## Instructions:\n",
                "1. **Ex√©cutez le premier bloc** (Importations)\n",
                "2. **Ex√©cutez le deuxi√®me bloc** (Configuration)\n",
                "3. **Allez au troisi√®me bloc**, entrez votre URL et ex√©cutez-le pour obtenir les r√©sultats\n",
                "4. **Optionnel**: Ex√©cutez le bloc BONUS pour sauvegarder les donn√©es scrap√©es en CSV"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üì¶ Bloc 1: Importations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Importations termin√©es avec succ√®s!\n"
                    ]
                }
            ],
            "source": [
                "# Import des biblioth√®ques n√©cessaires\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from fake_useragent import UserAgent\n",
                "import pandas as pd\n",
                "import json\n",
                "import time\n",
                "import random\n",
                "import os\n",
                "from urllib.parse import urlparse\n",
                "\n",
                "print(\"‚úÖ Importations termin√©es avec succ√®s!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ‚öôÔ∏è Bloc 2: Configuration du Scraper (Ne pas modifier)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Scraper configur√© et pr√™t √† l'emploi!\n"
                    ]
                }
            ],
            "source": [
                "class ModernScraper:\n",
                "    def __init__(self, use_random_agent=True):\n",
                "        self.session = requests.Session()\n",
                "        self.ua = UserAgent() if use_random_agent else None\n",
                "        self.use_random_agent = use_random_agent\n",
                "        self.headers = {\n",
                "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
                "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
                "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
                "            \"Connection\": \"keep-alive\",\n",
                "            \"Upgrade-Insecure-Requests\": \"1\",\n",
                "            \"Sec-Fetch-Dest\": \"document\",\n",
                "            \"Sec-Fetch-Mode\": \"navigate\",\n",
                "            \"Sec-Fetch-Site\": \"none\",\n",
                "            \"Sec-Fetch-User\": \"?1\",\n",
                "            \"Cache-Control\": \"max-age=0\",\n",
                "        }\n",
                "        self._update_headers()\n",
                "\n",
                "    def _update_headers(self):\n",
                "        if self.use_random_agent:\n",
                "            self.headers[\"User-Agent\"] = self.ua.random\n",
                "        else:\n",
                "            self.headers[\"User-Agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
                "        self.session.headers.update(self.headers)\n",
                "\n",
                "    def fetch(self, url, method=\"GET\", data=None, params=None, is_json=False):\n",
                "        self._update_headers()\n",
                "        \n",
                "        try:\n",
                "            time.sleep(random.uniform(1, 3))\n",
                "            \n",
                "            print(f\"üîç R√©cup√©ration de {url}...\")\n",
                "            \n",
                "            if method.upper() == \"GET\":\n",
                "                response = self.session.get(url, params=params, timeout=15)\n",
                "            elif method.upper() == \"POST\":\n",
                "                response = self.session.post(url, data=data, json=data, timeout=15)\n",
                "            else:\n",
                "                print(f\"‚ùå M√©thode non support√©e: {method}\")\n",
                "                return None\n",
                "\n",
                "            response.raise_for_status()\n",
                "\n",
                "            if is_json or \"application/json\" in response.headers.get(\"Content-Type\", \"\"):\n",
                "                print(\"‚úÖ R√©ponse JSON d√©tect√©e\")\n",
                "                return response.json()\n",
                "            \n",
                "            print(\"‚úÖ Page HTML r√©cup√©r√©e avec succ√®s\")\n",
                "            return BeautifulSoup(response.text, \"html.parser\")\n",
                "\n",
                "        except requests.exceptions.RequestException as e:\n",
                "            print(f\"‚ùå Erreur lors de la r√©cup√©ration: {e}\")\n",
                "            return None\n",
                "\n",
                "    def save_to_csv(self, data_list, filename=\"scraped_data.csv\"):\n",
                "        if not data_list:\n",
                "            print(\"‚ö†Ô∏è Aucune donn√©e √† sauvegarder\")\n",
                "            return\n",
                "\n",
                "        try:\n",
                "            output_dir = os.getcwd()\n",
                "            file_path = os.path.join(output_dir, filename)\n",
                "            \n",
                "            df = pd.DataFrame(data_list)\n",
                "            df.to_csv(file_path, mode='w', header=True, index=False)\n",
                "                \n",
                "            print(f\"‚úÖ Donn√©es sauvegard√©es dans: {file_path}\")\n",
                "            print(f\"üìä Nombre de lignes sauvegard√©es: {len(data_list)}\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Erreur lors de la sauvegarde: {e}\")\n",
                "\n",
                "# Initialisation du scraper\n",
                "scraper = ModernScraper()\n",
                "print(\"‚úÖ Scraper configur√© et pr√™t √† l'emploi!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üöÄ Bloc 3: Utilisation Simple - Entrez votre URL ici!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==================================================\n",
                        "üåê D√âBUT DU SCRAPING\n",
                        "==================================================\n",
                        "üîç R√©cup√©ration de https://httpbin.org/html...\n",
                        "‚úÖ Page HTML r√©cup√©r√©e avec succ√®s\n",
                        "\n",
                        "==================================================\n",
                        "üìä R√âSULTATS\n",
                        "==================================================\n",
                        "\n",
                        "üìÑ Titre de la page: Sans titre\n",
                        "\n",
                        "üìù Nombre de paragraphes: 1\n",
                        "üîó Nombre de liens: 0\n",
                        "\n",
                        "‚úÖ 2 √©l√©ments extraits de la page\n",
                        "\n",
                        "üìñ Aper√ßu des donn√©es extraites (5 premiers √©l√©ments):\n",
                        "  1. [paragraphe] Availing himself of the mild, summer-cool weather that now reigned in these lati...\n",
                        "  2. [titre_h1] Herman Melville - Moby-Dick...\n",
                        "\n",
                        "==================================================\n",
                        "‚úÖ SCRAPING TERMIN√â AVEC SUCC√àS!\n",
                        "==================================================\n",
                        "\n",
                        "üí° Vous pouvez maintenant ex√©cuter le bloc BONUS pour sauvegarder ces 2 donn√©es en CSV\n"
                    ]
                }
            ],
            "source": [
                "# ========================================\n",
                "# MODIFIEZ SEULEMENT CETTE PARTIE\n",
                "# ========================================\n",
                "\n",
                "# Entrez votre URL ici:\n",
                "url = \"https://httpbin.org/html\"  # ‚¨ÖÔ∏è Remplacez par votre URL\n",
                "\n",
                "# Type de requ√™te: \"GET\" ou \"POST\"\n",
                "method = \"GET\"  # ‚¨ÖÔ∏è Changez si n√©cessaire\n",
                "\n",
                "# ========================================\n",
                "# EX√âCUTION AUTOMATIQUE\n",
                "# ========================================\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"üåê D√âBUT DU SCRAPING\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# R√©cup√©ration des donn√©es\n",
                "result = scraper.fetch(url, method=method)\n",
                "\n",
                "# Variable pour stocker les donn√©es extraites\n",
                "scraped_data = []\n",
                "\n",
                "if result:\n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"üìä R√âSULTATS\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    # Si c'est du HTML\n",
                "    if isinstance(result, BeautifulSoup):\n",
                "        page_title = result.title.string if result.title else 'Sans titre'\n",
                "        print(f\"\\nüìÑ Titre de la page: {page_title}\")\n",
                "        print(f\"\\nüìù Nombre de paragraphes: {len(result.find_all('p'))}\")\n",
                "        print(f\"üîó Nombre de liens: {len(result.find_all('a'))}\")\n",
                "        \n",
                "        # Extraction automatique des donn√©es\n",
                "        # 1. Extraire tous les liens\n",
                "        links = result.find_all('a')\n",
                "        for link in links:\n",
                "            href = link.get('href', '')\n",
                "            text = link.get_text().strip()\n",
                "            if href and text:\n",
                "                scraped_data.append({\n",
                "                    'type': 'lien',\n",
                "                    'texte': text,\n",
                "                    'url': href,\n",
                "                    'page_source': url\n",
                "                })\n",
                "        \n",
                "        # 2. Extraire tous les paragraphes\n",
                "        paragraphs = result.find_all('p')\n",
                "        for i, p in enumerate(paragraphs, 1):\n",
                "            text = p.get_text().strip()\n",
                "            if text:\n",
                "                scraped_data.append({\n",
                "                    'type': 'paragraphe',\n",
                "                    'texte': text[:200],  # Limiter √† 200 caract√®res\n",
                "                    'position': i,\n",
                "                    'page_source': url\n",
                "                })\n",
                "        \n",
                "        # 3. Extraire les titres (h1, h2, h3)\n",
                "        for tag in ['h1', 'h2', 'h3']:\n",
                "            headers = result.find_all(tag)\n",
                "            for header in headers:\n",
                "                text = header.get_text().strip()\n",
                "                if text:\n",
                "                    scraped_data.append({\n",
                "                        'type': f'titre_{tag}',\n",
                "                        'texte': text,\n",
                "                        'page_source': url\n",
                "                    })\n",
                "        \n",
                "        # Afficher un aper√ßu des premi√®res donn√©es extraites\n",
                "        print(f\"\\n‚úÖ {len(scraped_data)} √©l√©ments extraits de la page\")\n",
                "        if scraped_data:\n",
                "            print(\"\\nüìñ Aper√ßu des donn√©es extraites (5 premiers √©l√©ments):\")\n",
                "            for i, item in enumerate(scraped_data[:5], 1):\n",
                "                print(f\"  {i}. [{item['type']}] {item['texte'][:80]}...\")\n",
                "    \n",
                "    # Si c'est du JSON\n",
                "    elif isinstance(result, dict):\n",
                "        print(\"\\nüîë Cl√©s JSON disponibles:\")\n",
                "        for key in result.keys():\n",
                "            print(f\"  - {key}\")\n",
                "        \n",
                "        # Convertir le JSON en format tabulaire\n",
                "        def flatten_json(data, parent_key=''):\n",
                "            items = []\n",
                "            if isinstance(data, dict):\n",
                "                for k, v in data.items():\n",
                "                    new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
                "                    if isinstance(v, (dict, list)):\n",
                "                        items.extend(flatten_json(v, new_key))\n",
                "                    else:\n",
                "                        items.append({'cle': new_key, 'valeur': str(v), 'page_source': url})\n",
                "            elif isinstance(data, list):\n",
                "                for i, item in enumerate(data):\n",
                "                    items.extend(flatten_json(item, f\"{parent_key}[{i}]\"))\n",
                "            return items\n",
                "        \n",
                "        scraped_data = flatten_json(result)\n",
                "        print(f\"\\n‚úÖ {len(scraped_data)} paires cl√©-valeur extraites\")\n",
                "        print(\"\\nüìÑ Aper√ßu des 5 premi√®res entr√©es:\")\n",
                "        for i, item in enumerate(scraped_data[:5], 1):\n",
                "            print(f\"  {i}. {item['cle']}: {item['valeur'][:60]}...\")\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*50)\n",
                "    print(\"‚úÖ SCRAPING TERMIN√â AVEC SUCC√àS!\")\n",
                "    print(\"=\"*50)\n",
                "    print(f\"\\nüí° Vous pouvez maintenant ex√©cuter le bloc BONUS pour sauvegarder ces {len(scraped_data)} donn√©es en CSV\")\n",
                "else:\n",
                "    print(\"\\n‚ùå √âchec du scraping. V√©rifiez l'URL et r√©essayez.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üíæ Bloc BONUS: Sauvegarder les donn√©es scrap√©es en CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==================================================\n",
                        "üíæ SAUVEGARDE DES DONN√âES\n",
                        "==================================================\n",
                        "\n",
                        "üìä Nombre de donn√©es √† sauvegarder: 2\n",
                        "‚úÖ Donn√©es sauvegard√©es dans: c:\\Users\\azizh\\OneDrive\\Bureau\\TekSpire 2.0\\data_scraping\\scraped_data.csv\n",
                        "üìä Nombre de lignes sauvegard√©es: 2\n",
                        "\n",
                        "‚úÖ Vous pouvez maintenant ouvrir le fichier CSV pour voir vos donn√©es!\n"
                    ]
                }
            ],
            "source": [
                "# ========================================\n",
                "# Ce bloc sauvegarde les donn√©es R√âELLES\n",
                "# extraites du Bloc 3\n",
                "# ========================================\n",
                "\n",
                "# Nom du fichier de sortie (vous pouvez le modifier)\n",
                "output_file = \"scraped_data.csv\"\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"üíæ SAUVEGARDE DES DONN√âES\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "if scraped_data:\n",
                "    print(f\"\\nüìä Nombre de donn√©es √† sauvegarder: {len(scraped_data)}\")\n",
                "    scraper.save_to_csv(scraped_data, output_file)\n",
                "    print(\"\\n‚úÖ Vous pouvez maintenant ouvrir le fichier CSV pour voir vos donn√©es!\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è Aucune donn√©e √† sauvegarder.\")\n",
                "    print(\"üí° Assurez-vous d'avoir ex√©cut√© le Bloc 3 d'abord!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Importations termin√©es avec succ√®s!\n",
                        "==================================================\n",
                        "üåê START SCRAPING\n",
                        "==================================================\n",
                        "üîç R√©cup√©ration de https://www.mayoclinic.org/diseases-conditions/crps-complex-regional-pain-syndrome/symptoms-causes/syc-20371151...\n",
                        "‚úÖ Page r√©cup√©r√©e avec succ√®s\n",
                        "‚úÖ 0 sections extraites\n",
                        "‚ö†Ô∏è Aucune donn√©e √† sauvegarder\n"
                    ]
                }
            ],
            "source": [
                "# ================================\n",
                "# IMPORTS\n",
                "# ================================\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from fake_useragent import UserAgent\n",
                "import json\n",
                "import time\n",
                "import random\n",
                "import os\n",
                "\n",
                "print(\"‚úÖ Importations termin√©es avec succ√®s!\")\n",
                "\n",
                "\n",
                "# ================================\n",
                "# SCRAPER CLASS\n",
                "# ================================\n",
                "class ModernScraper:\n",
                "    def __init__(self, use_random_agent=True):\n",
                "        self.session = requests.Session()\n",
                "        self.ua = UserAgent() if use_random_agent else None\n",
                "        self.use_random_agent = use_random_agent\n",
                "        self.headers = {\n",
                "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
                "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
                "            \"Connection\": \"keep-alive\",\n",
                "        }\n",
                "        self._update_headers()\n",
                "\n",
                "    def _update_headers(self):\n",
                "        if self.use_random_agent:\n",
                "            self.headers[\"User-Agent\"] = self.ua.random\n",
                "        else:\n",
                "            self.headers[\"User-Agent\"] = \"Mozilla/5.0\"\n",
                "        self.session.headers.update(self.headers)\n",
                "\n",
                "    def fetch(self, url):\n",
                "        try:\n",
                "            time.sleep(random.uniform(1, 2))\n",
                "            print(f\"üîç R√©cup√©ration de {url}...\")\n",
                "            response = self.session.get(url, timeout=15)\n",
                "            response.raise_for_status()\n",
                "            print(\"‚úÖ Page r√©cup√©r√©e avec succ√®s\")\n",
                "            return BeautifulSoup(response.text, \"html.parser\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Erreur: {e}\")\n",
                "            return None\n",
                "\n",
                "    def save_to_json(self, data_list, filename=\"output.json\"):\n",
                "        if not data_list:\n",
                "            print(\"‚ö†Ô∏è Aucune donn√©e √† sauvegarder\")\n",
                "            return\n",
                "        file_path = os.path.join(os.getcwd(), filename)\n",
                "        try:\n",
                "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "                json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
                "            print(f\"‚úÖ Donn√©es sauvegard√©es dans: {file_path}\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Erreur lors de la sauvegarde JSON: {e}\")\n",
                "\n",
                "\n",
                "# ================================\n",
                "# INITIALISATION\n",
                "# ================================\n",
                "scraper = ModernScraper()\n",
                "\n",
                "# üîΩ üîΩ üîΩ PUT YOUR URL HERE üîΩ üîΩ üîΩ\n",
                "url = \"https://www.mayoclinic.org/diseases-conditions/crps-complex-regional-pain-syndrome/symptoms-causes/syc-20371151\"\n",
                "# üîº üîº üîº CHANGE ONLY THIS üîº üîº üîº\n",
                "\n",
                "\n",
                "# ================================\n",
                "# SCRAPING\n",
                "# ================================\n",
                "print(\"=\"*50)\n",
                "print(\"üåê START SCRAPING\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "result = scraper.fetch(url)\n",
                "scraped_data = []\n",
                "\n",
                "if result:\n",
                "    h2_tags = result.find_all(\"h2\")\n",
                "\n",
                "    for h2 in h2_tags:\n",
                "        title = h2.get_text().strip()\n",
                "        paragraphs = []\n",
                "\n",
                "        # Get all siblings until next h2\n",
                "        for sibling in h2.find_next_siblings():\n",
                "            if sibling.name == \"h2\":\n",
                "                break\n",
                "            if sibling.name == \"p\":\n",
                "                text = sibling.get_text().strip()\n",
                "                if text:\n",
                "                    paragraphs.append(text)\n",
                "\n",
                "        if title and paragraphs:\n",
                "            scraped_data.append({\n",
                "                \"titre_h2\": title,\n",
                "                \"contenu\": \" \".join(paragraphs)\n",
                "            })\n",
                "\n",
                "    print(f\"‚úÖ {len(scraped_data)} sections extraites\")\n",
                "\n",
                "    # SAVE JSON\n",
                "    scraper.save_to_json(scraped_data, \"crregionalpainsyndrome.json\")\n",
                "\n",
                "else:\n",
                "    print(\"‚ùå Scraping failed\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# h2 + liste"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Importations termin√©es avec succ√®s!\n",
                        "==================================================\n",
                        "üåê START SCRAPING\n",
                        "==================================================\n",
                        "üîç R√©cup√©ration de https://www.pennmedicine.org/conditions/vaginal-cysts...\n",
                        "‚úÖ Page r√©cup√©r√©e avec succ√®s\n",
                        "‚úÖ 0 sections extraites\n",
                        "‚ö†Ô∏è Aucune donn√©e √† sauvegarder\n"
                    ]
                }
            ],
            "source": [
                "# ================================\n",
                "# IMPORTS\n",
                "# ================================\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from fake_useragent import UserAgent\n",
                "import json\n",
                "import time\n",
                "import random\n",
                "import os\n",
                "\n",
                "print(\"‚úÖ Importations termin√©es avec succ√®s!\")\n",
                "\n",
                "\n",
                "# ================================\n",
                "# SCRAPER CLASS\n",
                "# ================================\n",
                "class ModernScraper:\n",
                "    def __init__(self, use_random_agent=True):\n",
                "        self.session = requests.Session()\n",
                "        self.ua = UserAgent() if use_random_agent else None\n",
                "        self.use_random_agent = use_random_agent\n",
                "        self.headers = {\n",
                "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
                "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
                "            \"Connection\": \"keep-alive\",\n",
                "        }\n",
                "        self._update_headers()\n",
                "\n",
                "    def _update_headers(self):\n",
                "        if self.use_random_agent:\n",
                "            self.headers[\"User-Agent\"] = self.ua.random\n",
                "        else:\n",
                "            self.headers[\"User-Agent\"] = \"Mozilla/5.0\"\n",
                "        self.session.headers.update(self.headers)\n",
                "\n",
                "    def fetch(self, url):\n",
                "        try:\n",
                "            time.sleep(random.uniform(1, 2))\n",
                "            print(f\"üîç R√©cup√©ration de {url}...\")\n",
                "            response = self.session.get(url, timeout=15)\n",
                "            response.raise_for_status()\n",
                "            print(\"‚úÖ Page r√©cup√©r√©e avec succ√®s\")\n",
                "            return BeautifulSoup(response.text, \"html.parser\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Erreur: {e}\")\n",
                "            return None\n",
                "\n",
                "    def save_to_json(self, data_list, filename=\"output.json\"):\n",
                "        if not data_list:\n",
                "            print(\"‚ö†Ô∏è Aucune donn√©e √† sauvegarder\")\n",
                "            return\n",
                "        file_path = os.path.join(os.getcwd(), filename)\n",
                "        try:\n",
                "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "                json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
                "            print(f\"‚úÖ Donn√©es sauvegard√©es dans: {file_path}\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Erreur lors de la sauvegarde JSON: {e}\")\n",
                "\n",
                "\n",
                "# ================================\n",
                "# INITIALISATION\n",
                "# ================================\n",
                "scraper = ModernScraper()\n",
                "\n",
                "url = \"https://www.pennmedicine.org/conditions/vaginal-cysts\"\n",
                "\n",
                "\n",
                "# ================================\n",
                "# SCRAPING\n",
                "# ================================\n",
                "print(\"=\"*50)\n",
                "print(\"üåê START SCRAPING\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "result = scraper.fetch(url)\n",
                "scraped_data = []\n",
                "\n",
                "if result:\n",
                "    h2_tags = result.find_all(\"h2\")\n",
                "\n",
                "    for h2 in h2_tags:\n",
                "        title = h2.get_text().strip()\n",
                "        contents = []\n",
                "\n",
                "        # Parcours des √©l√©ments suivants jusqu'au prochain h2\n",
                "        for sibling in h2.find_next_siblings():\n",
                "            if sibling.name == \"h2\":\n",
                "                break\n",
                "\n",
                "            # Paragraphes\n",
                "            if sibling.name == \"p\":\n",
                "                text = sibling.get_text().strip()\n",
                "                if text:\n",
                "                    contents.append(text)\n",
                "\n",
                "            # Listes ul/li\n",
                "            if sibling.name == \"ul\":\n",
                "                for li in sibling.find_all(\"li\"):\n",
                "                    li_text = li.get_text().strip()\n",
                "                    if li_text:\n",
                "                        contents.append(li_text)\n",
                "\n",
                "        if title and contents:\n",
                "            scraped_data.append({\n",
                "                \"titre_h2\": title,\n",
                "                \"contenu\": \" \".join(contents)\n",
                "            })\n",
                "\n",
                "    print(f\"‚úÖ {len(scraped_data)} sections extraites\")\n",
                "\n",
                "    # SAVE JSON\n",
                "    scraper.save_to_json(scraped_data, \"vaginal cyst.json\")\n",
                "\n",
                "else:\n",
                "    print(\"‚ùå Scraping failed\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# h3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Importations termin√©es avec succ√®s!\n",
                        "==================================================\n",
                        "üåê START SCRAPING\n",
                        "==================================================\n",
                        "üîç R√©cup√©ration de https://my.clevelandclinic.org/health/diseases/23452-vaginal-cysts...\n",
                        "‚úÖ Page r√©cup√©r√©e avec succ√®s\n",
                        "‚úÖ 20 sections extraites\n",
                        "‚úÖ Donn√©es sauvegard√©es dans: c:\\Users\\azizh\\OneDrive\\Bureau\\MedCareAI\\data_scraping\\vaginal_cyst.json\n"
                    ]
                }
            ],
            "source": [
                "# ================================\n",
                "# IMPORTS\n",
                "# ================================\n",
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "from fake_useragent import UserAgent\n",
                "import json\n",
                "import time\n",
                "import random\n",
                "import os\n",
                "\n",
                "print(\"‚úÖ Importations termin√©es avec succ√®s!\")\n",
                "\n",
                "\n",
                "# ================================\n",
                "# SCRAPER CLASS\n",
                "# ================================\n",
                "class ModernScraper:\n",
                "    def __init__(self, use_random_agent=True):\n",
                "        self.session = requests.Session()\n",
                "        self.ua = UserAgent() if use_random_agent else None\n",
                "        self.use_random_agent = use_random_agent\n",
                "        self.headers = {\n",
                "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
                "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
                "            \"Connection\": \"keep-alive\",\n",
                "        }\n",
                "        self._update_headers()\n",
                "\n",
                "    def _update_headers(self):\n",
                "        if self.use_random_agent:\n",
                "            self.headers[\"User-Agent\"] = self.ua.random\n",
                "        else:\n",
                "            self.headers[\"User-Agent\"] = \"Mozilla/5.0\"\n",
                "        self.session.headers.update(self.headers)\n",
                "\n",
                "    def fetch(self, url):\n",
                "        try:\n",
                "            time.sleep(random.uniform(1, 2))\n",
                "            print(f\"üîç R√©cup√©ration de {url}...\")\n",
                "            response = self.session.get(url, timeout=15)\n",
                "            response.raise_for_status()\n",
                "            print(\"‚úÖ Page r√©cup√©r√©e avec succ√®s\")\n",
                "            return BeautifulSoup(response.text, \"html.parser\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Erreur: {e}\")\n",
                "            return None\n",
                "\n",
                "    def save_to_json(self, data_list, filename=\"output.json\"):\n",
                "        if not data_list:\n",
                "            print(\"‚ö†Ô∏è Aucune donn√©e √† sauvegarder\")\n",
                "            return\n",
                "        file_path = os.path.join(os.getcwd(), filename)\n",
                "        try:\n",
                "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "                json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
                "            print(f\"‚úÖ Donn√©es sauvegard√©es dans: {file_path}\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Erreur lors de la sauvegarde JSON: {e}\")\n",
                "\n",
                "\n",
                "# ================================\n",
                "# INITIALISATION\n",
                "# ================================\n",
                "scraper = ModernScraper()\n",
                "\n",
                "url = \"https://my.clevelandclinic.org/health/diseases/23452-vaginal-cysts\"\n",
                "\n",
                "\n",
                "# ================================\n",
                "# SCRAPING\n",
                "# ================================\n",
                "print(\"=\"*50)\n",
                "print(\"üåê START SCRAPING\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "result = scraper.fetch(url)\n",
                "scraped_data = []\n",
                "\n",
                "if result:\n",
                "    h3_tags = result.find_all(\"h3\")\n",
                "\n",
                "    for h3 in h3_tags:\n",
                "        title = h3.get_text().strip()\n",
                "        contents = []\n",
                "\n",
                "        # Parcours des √©l√©ments suivants jusqu'au prochain h3\n",
                "        for sibling in h3.find_next_siblings():\n",
                "            if sibling.name == \"h3\":\n",
                "                break\n",
                "\n",
                "            # Paragraphes\n",
                "            if sibling.name == \"p\":\n",
                "                text = sibling.get_text().strip()\n",
                "                if text:\n",
                "                    contents.append(text)\n",
                "\n",
                "            # Listes ul/li\n",
                "            if sibling.name == \"ul\":\n",
                "                for li in sibling.find_all(\"li\"):\n",
                "                    li_text = li.get_text().strip()\n",
                "                    if li_text:\n",
                "                        contents.append(li_text)\n",
                "\n",
                "        if title and contents:\n",
                "            scraped_data.append({\n",
                "                \"titre_h2\": title,\n",
                "                \"contenu\": \" \".join(contents)\n",
                "            })\n",
                "\n",
                "    print(f\"‚úÖ {len(scraped_data)} sections extraites\")\n",
                "\n",
                "    # SAVE JSON\n",
                "    scraper.save_to_json(scraped_data, \"vaginal_cyst.json\")\n",
                "\n",
                "else:\n",
                "    print(\"‚ùå Scraping failed\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
